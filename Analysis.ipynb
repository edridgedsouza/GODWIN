{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T00:00:03.387648Z",
     "start_time": "2021-03-24T00:00:02.712759Z"
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Dec 12 22:27:52 2014\n",
    "\n",
    "@author: LukasHalim\n",
    "Forked by @edridgedsouza\n",
    "\n",
    "2) For each post, determine earliest failure\n",
    "- Sort the comments for each post in ascending order\n",
    "- Create a variable for the comment number within each post\n",
    "- In cases where there is a failure, identify the first failing comment\n",
    "- In cases where there is not a failure, identify the final comment for the post\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#import csv\n",
    "\n",
    "import sqlite3\n",
    "from godwin import Scraper, Database\n",
    "\n",
    "db = Database('Godwin.db')\n",
    "# db.reset_db()\n",
    "s = Scraper(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-24T00:13:54.935Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping from /r/politicalcompassmemes: 100it [00:02, 48.19it/s]\n",
      "Scraped 1 of 124 subreddits\n",
      "Scraping from /r/europe: 100it [00:02, 49.26it/s]\n",
      "Scraped 2 of 124 subreddits\n",
      "Scraping from /r/teenagers: 100it [00:02, 46.35it/s]\n",
      "Scraped 3 of 124 subreddits\n",
      "Scraping from /r/witcher: 100it [00:01, 53.41it/s]\n",
      "Scraped 4 of 124 subreddits\n",
      "Scraping from /r/mechanicalkeyboards: 100it [00:01, 53.79it/s]\n",
      "Scraped 5 of 124 subreddits\n",
      "Scraping from /r/instagram: 100it [00:01, 86.23it/s]\n",
      "Scraped 6 of 124 subreddits\n",
      "Scraping from /r/moviedetails: 100it [00:01, 58.40it/s]\n",
      "Scraped 7 of 124 subreddits\n",
      "Scraping from /r/fo76: 100it [00:01, 50.69it/s]\n",
      "Scraped 8 of 124 subreddits\n",
      "Scraping from /r/whatcouldgowrong: 100it [00:02, 36.92it/s]\n",
      "Scraped 9 of 124 subreddits\n",
      "Scraping from /r/totalwar: 100it [00:01, 58.70it/s]\n",
      "Scraped 10 of 124 subreddits\n",
      "Scraping from /r/fortnitebr: 100it [00:02, 38.53it/s]\n",
      "Scraped 11 of 124 subreddits\n",
      "Scraping from /r/cringe: 100it [00:01, 97.78it/s]\n",
      "Scraped 12 of 124 subreddits\n",
      "Scraping from /r/barstoolsports: 100it [00:01, 93.04it/s]\n",
      "Scraped 13 of 124 subreddits\n",
      "Scraping from /r/cringetopia: 0it [00:00, ?it/s]\n",
      "Scraped 14 of 124 subreddits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subreddit cringetopia forbidden\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping from /r/facepalm: 100it [02:59,  1.79s/it]\n",
      "Scraped 15 of 124 subreddits\n",
      "Scraping from /r/books: 100it [02:34,  1.54s/it]\n",
      "Scraped 16 of 124 subreddits\n",
      "Scraping from /r/pokemon: 100it [02:47,  1.68s/it]\n",
      "Scraped 17 of 124 subreddits\n",
      "Scraping from /r/apexlegends: 100it [02:45,  1.65s/it]\n",
      "Scraped 18 of 124 subreddits\n",
      "Scraping from /r/dndnext: 100it [02:47,  1.68s/it]\n",
      "Scraped 19 of 124 subreddits\n",
      "Scraping from /r/trashy: 0it [00:00, ?it/s]\n",
      "Scraped 20 of 124 subreddits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subreddit trashy forbidden\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping from /r/sysadmin: 100it [02:30,  1.51s/it]\n",
      "Scraped 21 of 124 subreddits\n",
      "Scraping from /r/insanepeoplefacebook: 100it [02:33,  1.53s/it]\n",
      "Scraped 22 of 124 subreddits\n",
      "Scraping from /r/apple: 100it [02:22,  1.43s/it]\n",
      "Scraped 23 of 124 subreddits\n",
      "Scraping from /r/dataisbeautiful: 100it [02:37,  1.57s/it]\n",
      "Scraped 24 of 124 subreddits\n",
      "Scraping from /r/manga: 100it [02:21,  1.41s/it]\n",
      "Scraped 25 of 124 subreddits\n",
      "Scraping from /r/relationships: 100it [02:12,  1.32s/it]\n",
      "Scraped 26 of 124 subreddits\n",
      "Scraping from /r/iama: 88it [01:48,  1.23s/it]\n",
      "Scraped 27 of 124 subreddits\n",
      "Scraping from /r/valorant: 100it [02:21,  1.41s/it]\n",
      "Scraped 28 of 124 subreddits\n",
      "Scraping from /r/unexpected: 100it [02:38,  1.59s/it]\n",
      "Scraped 29 of 124 subreddits\n",
      "Scraping from /r/askwomen: 63it [01:30,  1.34s/it]"
     ]
    }
   ],
   "source": [
    "s.scrape_top_subreddits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T21:21:19.730169Z",
     "start_time": "2021-03-23T21:21:19.215000Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "x = requests.get('https://api.pushshift.io/reddit/comment/search',\n",
    "            params = {'link_id':'knnflb', \n",
    "                      'q':['keyboard OR keyboards'], # How to make inclusive????\n",
    "                      'size': 1, 'sort': 'asc'}) # Gives first result\n",
    "x.raise_for_status()\n",
    "x = x.json()['data']  # reverse so that oldest entries are first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T21:18:32.030281Z",
     "start_time": "2021-03-23T21:18:30.407722Z"
    }
   },
   "outputs": [],
   "source": [
    "y = requests.get('https://api.pushshift.io/reddit/submission/comment_ids/knnflb')\n",
    "y.raise_for_status()\n",
    "y = y.json()['data']  # Most recent entries are first here. So simply do y.index(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "comments_df = pd.read_sql(\"select * from comment\",conn)\n",
    "g = comments_df.groupby('post_id')\n",
    "#Sort the comments for each post in ascending order\n",
    "comments_df['RN'] = g['comment_created'].rank(method='first')\n",
    "#Create a variable for the comment number within each post\n",
    "comments_with_nazi_df = comments_df[comments_df.nazi_in_comment == 1]\n",
    "#Identify posts where there is a mention of Nazi\n",
    "nazi_posts = comments_with_nazi_df['post_id'].unique()\n",
    "\n",
    "#In cases where there is a comparison with Nazis or Hitler, identify the first comment \n",
    "#where the comparison is made\n",
    "mins = comments_with_nazi_df.groupby('post_id')['RN'].idxmin()\n",
    "first_nazi_comment = comments_with_nazi_df.loc[mins]\n",
    "\n",
    "right_censored_posts = comments_df[comments_df.post_id.isin(nazi_posts) == False]\n",
    "maxes = right_censored_posts.groupby('post_id')['RN'].idxmax()\n",
    "final_comment = right_censored_posts.loc[maxes]\n",
    "\n",
    "#combine the censored posts with those where a comparison is made\n",
    "concatenated = pd.concat([first_nazi_comment,final_comment])\n",
    "\n",
    "T = concatenated['RN']\n",
    "E = concatenated['nazi_in_comment']\n",
    "\n",
    "from lifelines import KaplanMeierFitter\n",
    "kmf = KaplanMeierFitter()\n",
    "kmf.fit(T, event_observed=E)\n",
    "kmf.plot()\n",
    "\n",
    "plt.xlim(0,2000);\n",
    "plt.title(\"Reddit Post Lifespan Prior to Mention of Nazi or Hitler\");\n",
    "plt.xlabel(\"Comments\")\n",
    "plt.ylabel(\"Fraciton of Posts Without Mention of Hitler or Nazis\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
